{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6639e47a-f763-4d70-847d-31f314803ea3",
   "metadata": {},
   "source": [
    "# Training the ConvNP Model \n",
    "\n",
    "In this notebook, we will preprocess Great Lakes data using an existing data processor, generate tasks for model training, and set up a training loop to train a **ConvNP** model using DeepSensor. We will:\n",
    "1. Load and preprocess temporal and static datasets like **SST**, **Ice Concentration**, **Lake Mask**, and **Bathymetry**.\n",
    "2. Load and use an existing **DataProcessor** to handle data normalization.\n",
    "3. Generate tasks using **TaskLoader** and train the **ConvNP** model.\n",
    "4. Monitor validation performance and track model training losses and RMSE (Root Mean Squared Error).\n",
    "\n",
    "Let's begin by importing necessary packages and defining helper functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e0a2b-f36f-468a-8d98-33ad8b99d830",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages and Define Helper Functions\n",
    "\n",
    "We import the libraries required for:\n",
    "- Data manipulation and visualization (`xarray`, `pandas`, `matplotlib`).\n",
    "- Geospatial operations (`cartopy`).\n",
    "- Efficient computation with Dask (`dask`).\n",
    "- DeepSensor for data processing and model training (`deepsensor`).\n",
    "\n",
    "Additionally, we import local helper functions such as `standardize_dates`, which standardizes the 'time' dimension in the dataset to a date-only format (`datetime64[D]`). We also define `generate_random_coordinates` and custom save and load functions, as the default functions in DeepSensor appear to be broken in this environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc27116-7a2e-4252-a198-b9d52c636659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import dask.array as da\n",
    "import gcsfs\n",
    "import os\n",
    "\n",
    "import deepsensor.torch\n",
    "from deepsensor.data import DataProcessor, TaskLoader, construct_circ_time_ds\n",
    "from deepsensor.data.sources import get_era5_reanalysis_data, get_earthenv_auxiliary_data, \\\n",
    "    get_gldas_land_mask\n",
    "from deepsensor.model import ConvNP\n",
    "from deepsensor.train import Trainer, set_gpu_default_device\n",
    "\n",
    "# Local package utilities\n",
    "from deepsensor_greatlakes.utils import standardize_dates, generate_random_coordinates, apply_mask_to_prediction\n",
    "from deepsensor_greatlakes.preprocessor import SeasonalCycleProcessor, list_saved_seasonal_cycles\n",
    "from deepsensor_greatlakes.model import save_model, load_convnp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9704bd8-859f-466f-bbec-e55b27c9b3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_gpu_default_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd8e06-fc32-474a-abf4-f44f46ab21e9",
   "metadata": {},
   "source": [
    "## Step 2: Data Inventory and Preprocessing\n",
    "\n",
    "In this section, we load the required environmental datasets for model training:\n",
    "- **Ice Concentration**: A dataset of ice cover over time in the Great Lakes.\n",
    "- **GLSEA (Sea Surface Temperature)**: A dataset of sea surface temperature.\n",
    "- **Bathymetry**: A dataset representing the underwater topography of the lakes.\n",
    "- **Lake Mask**: A binary mask indicating water presence.\n",
    "\n",
    "These datasets are loaded from storage and preprocessed by converting time into date-only format and handling missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07591068-d808-4126-ac5b-e76acd7b2436",
   "metadata": {},
   "source": [
    "### User Inputs - Select Training and Validation Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241495b1-96af-4768-9f4b-bd4610eebcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training/data config (adapted for Great Lakes)\n",
    "#data_range = (\"2009-01-01\", \"2022-12-31\")\n",
    "#train_range = (\"2009-01-01\", \"2021-12-31\")\n",
    "#val_range = (\"2022-01-01\", \"2022-12-31\")\n",
    "#date_subsample_factor = 10\n",
    "\n",
    "# Just two years for demo purposes\n",
    "data_range = (\"2009-01-01\", \"2010-12-31\")\n",
    "train_range = (\"2009-01-01\", \"2009-12-31\")\n",
    "val_range = (\"2010-01-01\", \"2010-12-31\")\n",
    "date_subsample_factor = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63571c-877a-4eb3-b8bc-3719baeaf258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the Zarr stores (NOTE: This won't work on U-M HPC. Paths must be changed)\n",
    "#bathymetry_path = 'gs://great-lakes-osd/context/interpolated_bathymetry.nc'\n",
    "#mask_path = 'gs://great-lakes-osd/context/lakemask.nc'\n",
    "#ice_concentration_path = 'gs://great-lakes-osd/ice_concentration.zarr'\n",
    "#glsea_path = 'gs://great-lakes-osd/GLSEA_combined.zarr'\n",
    "#glsea3_path = 'gs://great-lakes-osd/GLSEA3_combined.zarr'\n",
    "\n",
    "# Path to the files on U-M HPC\n",
    "bathymetry_path = '/nfs/turbo/seas-dannes/SST-sensor-placement-input/bathymetry/interpolated_bathymetry.nc'\n",
    "mask_path = '/nfs/turbo/seas-dannes/SST-sensor-placement-input/masks/lakemask.nc'\n",
    "ice_concentration_path = '/nfs/turbo/seas-dannes/SST-sensor-placement-input/ice_concentration_processed.zarr'\n",
    "glsea_path = '/nfs/turbo/seas-dannes/SST-sensor-placement-input/glsea_anom_processed.zarr'\n",
    "glsea_raw_path = '/nfs/turbo/seas-dannes/SST-sensor-placement-input/GLSEA3_combined.zarr'\n",
    "\n",
    "# Paths to saved configurations\n",
    "deepsensor_folder = '../deepsensor_config/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f1a2d3-2263-4076-9b92-a956ce383d70",
   "metadata": {},
   "source": [
    "## Step 3: Loading Temporal Datasets (Ice Concentration and GLSEA)\n",
    "\n",
    "In this section, we load the **Ice Concentration** and **GLSEA** datasets stored in Zarr format. These datasets contain critical temporal information on ice cover and sea surface temperature.\n",
    "\n",
    "We perform the following preprocessing:\n",
    "1. Replace invalid land values (denoted by `-1`) with `NaN`.\n",
    "2. Standardize the time dimension to date-only precision.\n",
    "3. Drop unnecessary variables like **CRS**.\n",
    "\n",
    "Letâ€™s load and preprocess the data now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec454436-7e94-4c99-8455-851dee1d3d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the Zarr stores\n",
    "ice_concentration = xr.open_zarr(ice_concentration_path)\n",
    "glsea = xr.open_zarr(glsea_path)\n",
    "glsea_raw = xr.open_zarr(glsea_raw_path)\n",
    "\n",
    "# Replace -1 (land value) with NaN\n",
    "ice_concentration = ice_concentration.where(ice_concentration != -1, float('nan'))\n",
    "\n",
    "# Convert all times to date-only format, removing the time component\n",
    "ice_concentration = standardize_dates(ice_concentration)\n",
    "glsea = standardize_dates(glsea)\n",
    "#glsea3 = standardize_dates(glsea3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6663bb-aca3-4178-95a7-018fee440717",
   "metadata": {},
   "source": [
    "## Step 4: Loading Static Datasets (Bathymetry and Lake Mask)\n",
    "\n",
    "Next, we load two static datasets:\n",
    "- **Bathymetry**: The underwater features of the Great Lakes.\n",
    "- **Lake Mask**: A binary mask indicating water bodies within the lakes.\n",
    "\n",
    "These datasets are loaded from NetCDF files and undergo basic preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c46d5-9ca6-4481-9531-e27c11b2ea1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the NetCDF files using xarray \n",
    "bathymetry_raw = xr.open_dataset(bathymetry_path)\n",
    "lakemask_raw = xr.open_dataset(mask_path)\n",
    "\n",
    "# Name the bathymetry variable (only needed if reading from GCP)\n",
    "#bathymetry_raw = bathymetry_raw.rename({'__xarray_dataarray_variable__': 'bathymetry'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7979c0-a43f-4f8c-a177-058d5dfbce6c",
   "metadata": {},
   "source": [
    "## Step 5: Initialize the Data Processor\n",
    "\n",
    "The **DataProcessor** from DeepSensor is used to preprocess and normalize the datasets, getting them ready for model training. It applies scaling and transformation techniques to the datasets, such as **min-max scaling**.\n",
    "\n",
    "We initialize the **DataProcessor** and apply it to the datasets. Below we load the `data_processor` that we fit in the last notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b872b-ed35-41d0-ab92-777fe052197c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_processor = DataProcessor(deepsensor_folder)\n",
    "data_processor = DataProcessor(os.path.join(deepsensor_folder, \"data_processor\"))\n",
    "print(data_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cafa26-910a-41c2-a46b-9d7f7058fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glsea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75be246-925b-4c49-8734-f92742fd14f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process the bathymetry and lake\n",
    "bathymetry, lakemask = data_processor([bathymetry_raw, lakemask_raw], method=\"min_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e8429-6c23-472d-b119-f336dd3d01ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_processor.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee20682-a5f3-4196-b06d-db17277ac04c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates = pd.date_range(glsea.time.values.min(), glsea.time.values.max(), freq=\"D\")\n",
    "dates = pd.to_datetime(dates).normalize()  # This will set all times to 00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f11be2-b77e-4444-ba92-1563b29adf18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doy_ds = construct_circ_time_ds(dates, freq=\"D\")\n",
    "cos_D = standardize_dates(doy_ds[\"cos_D\"])\n",
    "sin_D = standardize_dates(doy_ds[\"sin_D\"])\n",
    "print(cos_D)\n",
    "print(sin_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38acbd3-de41-445d-a120-ed86dda5252a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6: Generating Random Coordinates within the Lake Mask\n",
    "\n",
    "We generate random coordinates within the **lake mask**. These coordinates represent sampling points inside the Great Lakes region. The **DataProcessor** is used to normalize these coordinates, ensuring that they are suitable for training the model.\n",
    "\n",
    "We will generate `N` random coordinates and plot them to visualize their distribution within the lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8789c-a205-4d66-a597-ed123cdeb38c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "N = 100  # Number of random points\n",
    "random_lake_points = generate_random_coordinates(lakemask_raw, N, data_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc0577-5a36-45f4-b4e8-2a849b90f47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming random_coords is the (2, N) array from the previous step\n",
    "latitudes = random_lake_points[0, :]\n",
    "longitudes = random_lake_points[1, :]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(longitudes, latitudes, color='blue', alpha=0.5, s=10)\n",
    "plt.title(\"Scatter plot of N Random Coordinates within Lake Mask\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07199e49-9de7-4d81-8e17-8bdcafb2049c",
   "metadata": {},
   "source": [
    "## Step 7: Task Generation for Model Training\n",
    "\n",
    "In this section, we use **TaskLoader** to generate tasks. A task consists of context data (input features like sea surface temperature, bathymetry, etc.) and target data (what we want the model to predict, such as ice concentration).\n",
    "\n",
    "We generate tasks for training by sampling from the datasets. Each task represents a training example that the model will learn from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c1497-16fe-49eb-8a29-494e4b7ff7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make auxiliary context dataset\n",
    "aux_ds = xr.Dataset({\n",
    "    \"lakemask\": lakemask[\"mask\"],\n",
    "    \"bathymetry\": bathymetry[\"bathymetry\"],\n",
    "    \"cos_D\": cos_D,\n",
    "    \"sin_D\": sin_D,\n",
    "})\n",
    "\n",
    "# Initialize task loader\n",
    "task_loader = TaskLoader(\n",
    "    context = [glsea, ice_concentration, aux_ds],\n",
    "    target = glsea,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed0bda-ff9c-4f79-a460-71a08e6be2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4c806-0add-476d-8c6e-91ac30828e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function to generate tasks\n",
    "def gen_tasks(dates, N=100, progress=True, lakemask_raw=None, data_processor=None):\n",
    "    if lakemask_raw is None or data_processor is None:\n",
    "        raise ValueError(\"You must pass both `lakemask_raw` and `data_processor`.\")\n",
    "\n",
    "    tasks = []\n",
    "    for date in tqdm(dates, disable=not progress):\n",
    "        # Generate a fresh set of random lake points for each date\n",
    "        random_points = generate_random_coordinates(lakemask_raw, N, data_processor)\n",
    "        \n",
    "        # Sample the task\n",
    "        task = task_loader(date, context_sampling=random_points, target_sampling=\"all\")\n",
    "        \n",
    "        # Remove NaNs from the target\n",
    "        task = task.remove_target_nans()\n",
    "        \n",
    "        tasks.append(task)\n",
    "    \n",
    "    return tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a0895-1ae8-46c6-9d6f-812dd2c64149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate training and validation dates\n",
    "train_dates = pd.date_range(train_range[0], train_range[1])[::date_subsample_factor]\n",
    "val_dates = pd.date_range(val_range[0], val_range[1])[::date_subsample_factor]\n",
    "\n",
    "# Normalize to datetime64[D]\n",
    "train_dates = pd.to_datetime(train_dates).normalize()\n",
    "val_dates = pd.to_datetime(val_dates).normalize()\n",
    "\n",
    "# Generate training and validation tasks\n",
    "train_tasks = gen_tasks(\n",
    "    train_dates,\n",
    "    N=100,  # or whatever number of lake points you want per task\n",
    "    lakemask_raw=lakemask_raw,\n",
    "    data_processor=data_processor\n",
    ")\n",
    "\n",
    "val_tasks = gen_tasks(\n",
    "    val_dates,\n",
    "    N=100,  # match N for validation, or use a different number if you want\n",
    "    lakemask_raw=lakemask_raw,\n",
    "    data_processor=data_processor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597325e-44ff-4ec2-b1e7-49f52ac6e666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tasks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76976714-6f12-4190-9b80-81ef40ba992b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = deepsensor.plot.task(train_tasks[2], task_loader)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51021897-2922-4b25-abe1-2a5d765a21a6",
   "metadata": {},
   "source": [
    "## Step 8: Model Setup and Training\n",
    "\n",
    "We now set up the **ConvNP** model, a neural process-based model from **DeepSensor**. We use the **DataProcessor** and **TaskLoader** as inputs to the model, which allows the model to handle context and target data properly during training.\n",
    "\n",
    "The model is then trained for a set number of epochs, and we monitor its performance by tracking the training loss and validation RMSE (Root Mean Squared Error).\n",
    "\n",
    "At the end of the training loop, we save the best-performing model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8e743-19a4-49df-b934-74377c7c6444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model = ConvNP(data_processor, task_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241a23d-3065-4093-8608-d03e2718f0bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Trainer and training loop\n",
    "trainer = Trainer(model, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a64c4-d45a-413f-a61d-fda87786270d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Monitor validation performance\n",
    "def compute_val_rmse(model, val_tasks):\n",
    "    errors = []\n",
    "    target_var_ID = task_loader.target_var_IDs[0][0]  # assuming 1st target set and 1D\n",
    "    \n",
    "    for task in val_tasks:\n",
    "        with torch.no_grad():\n",
    "            mean = data_processor.map_array(model.mean(task), target_var_ID, unnorm=True)\n",
    "            true = data_processor.map_array(task[\"Y_t\"][0], target_var_ID, unnorm=True)\n",
    "            \n",
    "        errors.extend((mean - true) ** 2)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return np.sqrt(np.mean(np.concatenate(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f20e2-3e6d-4351-ac17-f9afa715c579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Track the losses and validation RMSEs\n",
    "losses = []\n",
    "val_rmses = []\n",
    "val_rmse_best = np.inf\n",
    "\n",
    "# Start the training loop\n",
    "for epoch in tqdm(range(50), desc=\"Training Epochs\"):  # Training for 50 epochs\n",
    "    # Generate tasks for training\n",
    "    batch_losses = trainer(train_tasks)\n",
    "    losses.append(np.mean(batch_losses))\n",
    "\n",
    "    # Compute the validation RMSE\n",
    "    val_rmse = compute_val_rmse(model, val_tasks)\n",
    "    val_rmses.append(val_rmse)\n",
    "\n",
    "    # Save the model if it performs better\n",
    "    if val_rmse < val_rmse_best:\n",
    "        val_rmse_best = val_rmse\n",
    "        #model.save(deepsensor_folder)\n",
    "        save_model(model, deepsensor_folder)\n",
    "\n",
    "# Plot training losses and validation RMSE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_title('Training Loss')\n",
    "\n",
    "axes[1].plot(val_rmses)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_title('Validation RMSE')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af31e1-fd09-4e46-9e0a-de6dfd412bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load it later:\n",
    "# Assuming you have data_processor and task_loader instantiated in your notebook\n",
    "loaded_model = load_convnp_model(deepsensor_folder, data_processor, task_loader)\n",
    "print(\"Model loaded successfully with custom function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361de480-b534-4e65-8c4c-325bbdaf30d1",
   "metadata": {},
   "source": [
    "## Step 9: Prediction\n",
    "\n",
    "Now that we have a trained model, we can use it to make a prediction. Notice that we get both a mean and standard deviation from this prediciton. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7f2f1-ceb5-49f1-b262-66f132030f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = model\n",
    "\n",
    "date = \"2010-08-29\"\n",
    "test_task = task_loader(date, context_sampling=random_lake_points, target_sampling=\"all\")\n",
    "prediction_ds = loaded_model.predict(test_task, X_t=glsea)\n",
    "prediction_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a1184-76be-4c63-bc54-f3d3828ecc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ds_masked = apply_mask_to_prediction(prediction_ds['sst'], lakemask_raw)\n",
    "prediction_ds_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d524f5-4d18-436f-9660-719bc785dc76",
   "metadata": {},
   "source": [
    "Note that the prediction produces both a mean prediction and a standard deviation, which is a characteristic of a Gaussian Process approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0107b1-04d0-4094-931c-6f879ef49954",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "prediction_ds_masked['mean'].plot(cmap='viridis', cbar_kwargs={'label': 'Predicted Mean SST'})\n",
    "plt.title(f'Masked Predicted Mean SST for Single Day')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "prediction_ds_masked['std'].plot(cmap='plasma', cbar_kwargs={'label': 'Predicted Std SST'})\n",
    "plt.title(f'Masked Predicted Std SST for Single Day')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e5a47-83ce-448c-ba41-537cbd7d16e4",
   "metadata": {},
   "source": [
    "The above plot looks really bizarre because it has only been trained on two years of data! DeepSensor's models are data hungry..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb4cd3-1987-4876-b7ac-29546d9203b1",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we:\n",
    "1. Loaded and preprocessed several Great Lakes datasets for training a **ConvNP** model.\n",
    "2. Generated tasks using **TaskLoader** and visualized data to perform sanity checks.\n",
    "3. Trained the **ConvNP** model and monitored its performance.\n",
    "\n",
    "Next, we will explore the active learning component of **DeepSensor**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70af45-daec-4005-81fb-c1f620e75c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "venv",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
