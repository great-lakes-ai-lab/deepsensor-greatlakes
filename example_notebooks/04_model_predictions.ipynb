{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd30c4a-f175-4a13-95c4-61b6f13da021",
   "metadata": {},
   "source": [
    "# Prediction with Trained ConvNP Model\n",
    "\n",
    " This notebook demonstrates how to load a pre-trained ConvNP model,\n",
    " perform predictions on new data, and visualize the model's mean\n",
    " and standard deviation predictions for Great Lakes SST.\n",
    "\n",
    " We will:\n",
    " 1. Load the pre-trained ConvNP model from disk.\n",
    " 2. Prepare a prediction task for a specific date, including context\n",
    "    (e.g., random sensor observations) and the full lake grid as target.\n",
    " 3. Use the model to generate mean and standard deviation predictions.\n",
    " 4. Apply the lake mask to the predictions for clear visualization.\n",
    " 5. Create and save high-quality plots of the masked predictions.\n",
    " 6. Perform a time series prediction for a specific point in Lake Superior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e195b-3cde-45ea-957b-09b0c9109344",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages and Set Up Environment\n",
    "\n",
    " We import necessary libraries for data handling, plotting, and DeepSensor.\n",
    " We also make sure that the GPU is set as the default device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7783eff9-eb6e-4144-b062-ff67adf69710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs # For geographical plots\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "import sys # For sys.stdout.flush()\n",
    "from tqdm import tqdm # For progress bar in time series loop\n",
    "\n",
    "import deepsensor.torch\n",
    "from deepsensor.data import DataProcessor, TaskLoader, construct_circ_time_ds\n",
    "from deepsensor.model import ConvNP\n",
    "from deepsensor.train import set_gpu_default_device\n",
    "from deepsensor.data import Task # Explicitly import Task for manual construction\n",
    "\n",
    "# Local package utilities\n",
    "from deepsensor_greatlakes.utils import standardize_dates, generate_random_coordinates, apply_mask_to_prediction\n",
    "\n",
    "# --- IMPORTANT: Make sure that load_convnp_model is correctly imported.\n",
    "# Assuming it's in deepsensor_greatlakes.model as previously.\n",
    "from deepsensor_greatlakes.model import load_convnp_model\n",
    "\n",
    "# --- Batch Mode: Define Output Directory ---\n",
    "PREDICTION_OUTPUT_DIR = 'prediction_plots' # Specific output directory for prediction results\n",
    "os.makedirs(PREDICTION_OUTPUT_DIR, exist_ok=True)\n",
    "# --- End Batch Mode Config ---\n",
    "\n",
    "set_gpu_default_device()\n",
    "\n",
    "print(\"Environment setup complete.\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd02e0-ca0f-46fc-b190-4908b8ba8f4f",
   "metadata": {},
   "source": [
    "## Step 2: User Inputs - Paths and Prediction Dates/Locations\n",
    "\n",
    " Define the paths to your raw data and the folder where your trained model is saved.\n",
    " Also, specify the dates/locations for which you want to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40e2a45-2075-44f1-9349-0c46e79f9b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single prediction configured for date: 2022-07-15 with 100 context points.\n",
      "Time series prediction configured for Lat: 47.5, Lon: -87.5 from 2022-01-01 to 2022-12-31.\n"
     ]
    }
   ],
   "source": [
    "# Paths to the files on U-M HPC\n",
    "bathymetry_path = '/nfs/turbo/seas-dannes/SST-sensor-placement-input/bathymetry/interpolated_bathymetry.nc'\n",
    "mask_path = '/nfs/turbo/seas-dannes/SST-sensor-placement-input/masks/lakemask.nc'\n",
    "glsea_path = '/nfs/turbo/seas-dannes/SST-sensor-placement-input/GLSEA_combined.zarr'\n",
    "\n",
    "# Path to your trained model's folder\n",
    "# --- IMPORTANT: This should point to the output of your training script, e.g., run00/ ---\n",
    "model_folder_path = \"../saved_models/run00/\" # Assuming model is saved here\n",
    "deepsensor_config_path = \"../deepsensor_config/\" # Path to DataProcessor config\n",
    "\n",
    "# Date for a single full-map prediction\n",
    "SINGLE_PREDICTION_DATE = \"2022-07-15\" # Example date from the val_range \"2022-01-01\" to \"2022-12-31\"\n",
    "\n",
    "# Number of random context points to simulate sparse observations for all predictions\n",
    "N_CONTEXT_POINTS = 100\n",
    "\n",
    "# --- NEW: Time Series Prediction Configuration ---\n",
    "TARGET_LON_TS = -87.5 # Approximate middle of Lake Superior\n",
    "TARGET_LAT_TS = 47.5 # Approximate middle of Lake Superior\n",
    "TS_START_DATE = \"2022-01-01\"\n",
    "TS_END_DATE = \"2022-12-31\" # Using your validation range\n",
    "# --- END NEW ---\n",
    "\n",
    "print(f\"Single prediction configured for date: {SINGLE_PREDICTION_DATE} with {N_CONTEXT_POINTS} context points.\")\n",
    "print(f\"Time series prediction configured for Lat: {TARGET_LAT_TS}, Lon: {TARGET_LON_TS} from {TS_START_DATE} to {TS_END_DATE}.\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d747999-8867-427a-8aaf-62a88452c64e",
   "metadata": {},
   "source": [
    "## Step 3: Load Raw Data and Preprocess\n",
    "\n",
    " We load the raw GLSEA (SST) data, bathymetry, and the lake mask. The data processor\n",
    " (whose configuration is loaded from `deepsensor_config_path`) will handle normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac2ef2f-16c7-4fd2-aa9e-d1216b061f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of glsea_raw after loading and initial processing: <class 'xarray.core.dataset.Dataset'>\n",
      "GLSEA dataset has 10221 unique available dates within its loaded time range.\n",
      "Raw data loaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "# Open the Zarr store for GLSEA data\n",
    "glsea_raw = xr.open_zarr(glsea_path, chunks={'time': 366, 'lat': 200, 'lon': 200})\n",
    "glsea_raw = standardize_dates(glsea_raw)\n",
    "if 'crs' in glsea_raw.variables: # Check if 'crs' variable exists before dropping\n",
    "    glsea_raw = glsea_raw.drop_vars('crs')\n",
    "\n",
    "print(f\"Type of glsea_raw after loading and initial processing: {type(glsea_raw)}\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Open the NetCDF files for bathymetry and lake mask\n",
    "bathymetry_raw = xr.open_dataset(bathymetry_path)\n",
    "lakemask_raw = xr.open_dataset(mask_path)\n",
    "\n",
    "# --- NEW: Create a set of available dates from glsea_raw for fast lookup ---\n",
    "glsea_available_dates = set(glsea_raw['time'].values.astype('datetime64[D]'))\n",
    "print(f\"GLSEA dataset has {len(glsea_available_dates)} unique available dates within its loaded time range.\")\n",
    "sys.stdout.flush()\n",
    "# --- END NEW ---\n",
    "\n",
    "print(\"Raw data loaded and preprocessed.\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7bd8fd-b3cd-47b6-ad27-8a8c2af168e6",
   "metadata": {},
   "source": [
    "## Step 4: Initialize DataProcessor and TaskLoader\n",
    "\n",
    " The `DataProcessor` is essential for normalizing data, and the `TaskLoader`\n",
    " for creating prediction tasks consistent with how the model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e90060e-6ea8-43e9-bcad-3a8536c5d0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataProcessor initialized: DataProcessor with normalisation params:\n",
      "{'bathymetry': {'method': 'min_max',\n",
      "                'params': {'max': 316.62872313037894,\n",
      "                           'min': 9.999999999999998}},\n",
      " 'coords': {'time': {'name': 'time'},\n",
      "            'x1': {'map': (38.8749871947229, 55.4132976408956), 'name': 'lat'},\n",
      "            'x2': {'map': (-92.4199507342304, -75.8816402880577),\n",
      "                   'name': 'lon'}},\n",
      " 'mask': {'method': 'min_max', 'params': {'max': 1.0, 'min': 0.0}},\n",
      " 'sst': {'method': 'mean_std',\n",
      "         'params': {'mean': 7.873531818389893, 'std': 6.944828510284424}}}\n",
      "TaskLoader initialized: TaskLoader(3 context sets, 1 target sets)\n",
      "Context variable IDs: (('sst',), ('bathymetry', 'cos_D', 'sin_D'), ('mask',))\n",
      "Target variable IDs: (('sst',),)\n"
     ]
    }
   ],
   "source": [
    "data_processor = DataProcessor(deepsensor_config_path)\n",
    "print(\"DataProcessor initialized:\", data_processor)\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Process auxiliary data (like bathymetry, and day-of-year features)\n",
    "dates_full_range = pd.date_range(glsea_raw.time.values.min(), glsea_raw.time.values.max(), freq=\"D\")\n",
    "dates_full_range = pd.to_datetime(dates_full_range).normalize()\n",
    "\n",
    "doy_ds = construct_circ_time_ds(dates_full_range, freq=\"D\")\n",
    "\n",
    "# These were context variables during training, so they need to be processed here\n",
    "aux_ds_for_taskloader, lakemask_ds_for_taskloader = data_processor([bathymetry_raw, lakemask_raw], method=\"min_max\")\n",
    "aux_ds_for_taskloader[\"cos_D\"] = standardize_dates(doy_ds[\"cos_D\"])\n",
    "aux_ds_for_taskloader[\"sin_D\"] = standardize_dates(doy_ds[\"sin_D\"])\n",
    "\n",
    "# Process glsea data. This will be a DataArray or Dataset depending on glsea_raw.\n",
    "glsea_processed = data_processor(glsea_raw)\n",
    "\n",
    "# TaskLoader context sets must precisely match your training setup\n",
    "task_loader = TaskLoader(context=[glsea_processed, aux_ds_for_taskloader, lakemask_ds_for_taskloader], target=glsea_processed)\n",
    "print(\"TaskLoader initialized:\", task_loader)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246798d-d689-4d81-8885-0315226e450f",
   "metadata": {},
   "source": [
    "## Step 5: Load Trained Model\n",
    "\n",
    "Load the ConvNP model weights and configuration from your saved `model_folder_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aeeaf31-2cb3-4f64-8326-ab4e8524d2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to instantiate ConvNP model (randomly initialized initially):\n",
      "Architectural config for construct_neural_process (passed as **kwargs): {'dim_x': 2, 'dim_yc': [1, 3, 1], 'dim_yt': 1, 'dim_aux_t': 0, 'dim_lv': 0, 'conv_arch': 'unet', 'unet_channels': [64, 64, 64, 64], 'unet_resize_convs': True, 'unet_resize_conv_interp_method': 'bilinear', 'aux_t_mlp_layers': None, 'likelihood': 'het', 'unet_kernels': 5, 'internal_density': 1180, 'encoder_scales': [0.0004237288, 0.0009355429, 0.0004237288], 'encoder_scales_learnable': False, 'decoder_scale': 0.000847457627118644, 'decoder_scale_learnable': False, 'num_basis_functions': 64, 'epsilon': 0.01}\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model = load_convnp_model(model_folder_path, data_processor, task_loader)\n",
    "print(\"Model loaded successfully!\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed644c-c58f-451c-8ac8-99bab8bfb9f7",
   "metadata": {},
   "source": [
    "## Step 6: Single Full-Map Prediction\n",
    "\n",
    " A `Task` object is created for the `SINGLE_PREDICTION_DATE`.\n",
    " We'll use `N_CONTEXT_POINTS` random locations within the lake as context points,\n",
    " and the full `glsea_raw` grid as the target locations (`X_t`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf9f21a-a9ec-45af-b49c-5148592d62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Full-Map Prediction for 2022-07-15 ---\n",
      "Prediction task created for 2022-07-15.\n",
      "DEBUG: Type of prediction_task['X_c']: <class 'list'>\n",
      "DEBUG: Type of prediction_task['X_c'][0]: <class 'numpy.ndarray'>\n",
      "Number of context points: 2\n",
      "DEBUG: Type of prediction_task['X_t']: <class 'list'>\n",
      "DEBUG: Type of prediction_task['X_t'][0]: <class 'tuple'>\n",
      "Number of target points: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dannes/ds_env_gpu/lib/python3.10/site-packages/lab/types.py:178: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numerictypes.\n",
      "  for name in np.core.numerictypes.__all__ + [\"bool\"]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-map prediction completed.\n",
      "Prediction({'sst': ('mean', 'std')}), mode=on-grid\n",
      "Saved: prediction_plots/mean_sst_prediction_2022-07-15.png\n",
      "Saved: prediction_plots/std_sst_prediction_2022-07-15.png\n",
      "Full-map prediction section completed.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Starting Full-Map Prediction for {SINGLE_PREDICTION_DATE} ---\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Filter SINGLE_PREDICTION_DATE against glsea_available_dates\n",
    "if np.datetime64(SINGLE_PREDICTION_DATE, 'D') not in glsea_available_dates:\n",
    "    print(f\"Warning: SINGLE_PREDICTION_DATE {SINGLE_PREDICTION_DATE} not found in GLSEA data. Skipping full-map prediction.\")\n",
    "    sys.stdout.flush()\n",
    "else:\n",
    "    # Generate random context points within the lake mask for the prediction date\n",
    "    random_lake_points_for_prediction = generate_random_coordinates(lakemask_raw, N_CONTEXT_POINTS, data_processor)\n",
    "\n",
    "    # Create the prediction task\n",
    "    prediction_task = task_loader(\n",
    "        SINGLE_PREDICTION_DATE,\n",
    "        context_sampling=random_lake_points_for_prediction, # Use the N random points as context\n",
    "        target_sampling=\"all\" # Predict over the entire grid\n",
    "    )\n",
    "    prediction_task = prediction_task.remove_context_nans() # Ensure no NaNs in context\n",
    "\n",
    "    print(f\"Prediction task created for {SINGLE_PREDICTION_DATE}.\")\n",
    "    print(f\"DEBUG: Type of prediction_task['X_c']: {type(prediction_task['X_c'])}\")\n",
    "    if isinstance(prediction_task['X_c'], list) and len(prediction_task['X_c']) > 0:\n",
    "        print(f\"DEBUG: Type of prediction_task['X_c'][0]: {type(prediction_task['X_c'][0])}\")\n",
    "        print(f\"Number of context points: {len(prediction_task['X_c'][0])}\")\n",
    "    else:\n",
    "        print(\"DEBUG: prediction_task['X_c'] is not a list or is empty for context points.\")\n",
    "\n",
    "    print(f\"DEBUG: Type of prediction_task['X_t']: {type(prediction_task['X_t'])}\")\n",
    "    if isinstance(prediction_task['X_t'], list) and len(prediction_task['X_t']) > 0:\n",
    "        print(f\"DEBUG: Type of prediction_task['X_t'][0]: {type(prediction_task['X_t'][0])}\")\n",
    "        print(f\"Number of target points: {len(prediction_task['X_t'][0])}\")\n",
    "    else:\n",
    "        print(\"DEBUG: prediction_task['X_t'] is not a list or is empty for target points.\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "    # Perform Prediction\n",
    "    # X_t argument to model.predict should match the data structure used for training.\n",
    "    # It will typically be the raw Xarray object that defines the target grid.\n",
    "    prediction_ds = model.predict(prediction_task, X_t=glsea_raw)\n",
    "\n",
    "    print(\"Full-map prediction completed.\")\n",
    "    print(prediction_ds)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Apply Lake Mask and Save Plots\n",
    "    # Pass the entire Dataset for the 'sst' variable to the masking function.\n",
    "    # The 'prediction_ds' returned by model.predict is a dict like {'sst': Dataset_with_mean_and_std}\n",
    "    # The apply_mask_to_prediction function expects a Dataset as its first argument.\n",
    "    masked_prediction_output_ds = apply_mask_to_prediction(\n",
    "        prediction_ds['sst'], lakemask_raw\n",
    "    )\n",
    "    # Now extract the masked mean and std DataArrays from the returned Dataset.\n",
    "    prediction_ds_masked_mean = masked_prediction_output_ds['mean']\n",
    "    prediction_ds_masked_std = masked_prediction_output_ds['std']\n",
    "\n",
    "    # Plotting the masked mean prediction\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax_mean = plt.axes(projection=ccrs.PlateCarree())\n",
    "    prediction_ds_masked_mean.plot(\n",
    "        ax=ax_mean,\n",
    "        cmap='viridis',\n",
    "        cbar_kwargs={'label': 'Predicted Mean SST (°C)'}\n",
    "    )\n",
    "    ax_mean.add_feature(cfeature.COASTLINE)\n",
    "    ax_mean.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax_mean.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    ax_mean.add_feature(cfeature.RIVERS)\n",
    "    ax_mean.set_title(f'Masked Predicted Mean SST for {SINGLE_PREDICTION_DATE}')\n",
    "    ax_mean.set_xlabel('Longitude')\n",
    "    ax_mean.set_ylabel('Latitude')\n",
    "    plt.tight_layout()\n",
    "    mean_plot_filename = os.path.join(PREDICTION_OUTPUT_DIR, f'mean_sst_prediction_{SINGLE_PREDICTION_DATE}.png')\n",
    "    plt.savefig(mean_plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {mean_plot_filename}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "    # Plotting the masked standard deviation prediction\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax_std = plt.axes(projection=ccrs.PlateCarree())\n",
    "    prediction_ds_masked_std.plot(\n",
    "        ax=ax_std,\n",
    "        cmap='plasma', # A good cmap for uncertainty\n",
    "        cbar_kwargs={'label': 'Predicted Std SST (°C)'}\n",
    "    )\n",
    "    ax_std.add_feature(cfeature.COASTLINE)\n",
    "    ax_std.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax_std.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    ax_std.add_feature(cfeature.RIVERS)\n",
    "    ax_std.set_title(f'Masked Predicted Std SST for {SINGLE_PREDICTION_DATE}')\n",
    "    ax_std.set_xlabel('Longitude')\n",
    "    ax_std.set_ylabel('Latitude')\n",
    "    plt.tight_layout()\n",
    "    std_plot_filename = os.path.join(PREDICTION_OUTPUT_DIR, f'std_sst_prediction_{SINGLE_PREDICTION_DATE}.png')\n",
    "    plt.savefig(std_plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {std_plot_filename}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"Full-map prediction section completed.\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8160eb-d7fa-4bab-b2aa-71234b6c4669",
   "metadata": {},
   "source": [
    "## Step 7: Time Series Prediction for a Single Point\n",
    "\n",
    " Predict mean and standard deviation over a time range for a specific geographical point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255b1584-7aa2-411a-8858-936f97ada6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Time Series Prediction ---\n",
      "Time series will be predicted for 365 dates out of 365 available in GLSEA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Time Series: 100%|██████████| 365/365 [11:59<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: prediction_plots/time_series_sst_prediction_LSuperior_47.5_-87.5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: prediction_plots/time_series_sst_prediction_LSuperior_47.5_-87.5.png\n",
      "Time series prediction completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Time Series Prediction ---\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "ts_dates_full = pd.date_range(TS_START_DATE, TS_END_DATE).normalize()\n",
    "\n",
    "# Filter ts_dates against glsea_available_dates\n",
    "ts_dates_filtered = [d for d in ts_dates_full if np.datetime64(d, 'D') in glsea_available_dates]\n",
    "print(f\"Time series will be predicted for {len(ts_dates_filtered)} dates out of {len(ts_dates_full)} available in GLSEA.\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "ts_means = []\n",
    "ts_stds = []\n",
    "actual_ts_dates = [] # To store dates for which prediction was successful\n",
    "\n",
    "# Define the single target point for the time series\n",
    "# This creates a Dataset with the 'sst' variable containing a single NaN value\n",
    "# at the target lat/lon, and a time coordinate that will be updated in the loop.\n",
    "target_point_ds_template = xr.Dataset(\n",
    "    {'sst': (['lat', 'lon'], [[np.nan]])},\n",
    "    coords={'lat': [TARGET_LAT_TS], 'lon': [TARGET_LON_TS]}\n",
    ")\n",
    "\n",
    "for date_dt64 in tqdm(ts_dates_filtered, desc=\"Predicting Time Series\"):\n",
    "    try:\n",
    "        # Create context: Use the predefined N_CONTEXT_POINTS, but sample Y_c from glsea_raw for this date\n",
    "        # context_data_for_date is already a Task object from task_loader\n",
    "        ts_task = task_loader(\n",
    "            date_dt64,\n",
    "            context_sampling=random_lake_points_for_prediction,\n",
    "            target_sampling=None # Not getting target for context creation\n",
    "        ).remove_context_nans()\n",
    "\n",
    "        # Create target for this specific date and point\n",
    "        current_target_point_ds = target_point_ds_template.copy()\n",
    "        current_target_point_ds['time'] = np.datetime64(date_dt64, 'D')\n",
    "\n",
    "        # Set the target locations (X_t) for this specific point and date on the existing Task object\n",
    "        ts_task.X_t = current_target_point_ds # This sets the X_t *within* the Task object\n",
    "\n",
    "        # Perform prediction. Explicitly pass X_t, as the model.predict method requires it.\n",
    "        ts_prediction_ds = model.predict(ts_task, X_t=current_target_point_ds)\n",
    "\n",
    "        # Extract values, unnormalize, and convert to scalar\n",
    "        mean_val = data_processor.map_array(ts_prediction_ds['sst']['mean'], 'sst', unnorm=True).item()\n",
    "        std_val = data_processor.map_array(ts_prediction_ds['sst']['std'], 'sst', unnorm=True).item()\n",
    "\n",
    "        ts_means.append(mean_val)\n",
    "        ts_stds.append(std_val)\n",
    "        actual_ts_dates.append(date_dt64)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not predict for date {date_dt64} due to error: {e}. Skipping.\")\n",
    "        sys.stdout.flush()\n",
    "        continue\n",
    "\n",
    "\n",
    "if not ts_means:\n",
    "    print(\"No data points successfully predicted for time series. Skipping plot.\")\n",
    "    sys.stdout.flush()\n",
    "else:\n",
    "    # Convert lists to pandas DataFrame for plotting\n",
    "    ts_df = pd.DataFrame({\n",
    "        'mean': ts_means,\n",
    "        'std': ts_stds\n",
    "    }, index=pd.to_datetime(actual_ts_dates))\n",
    "\n",
    "    # Save time series data to CSV\n",
    "    ts_csv_filename = os.path.join(PREDICTION_OUTPUT_DIR, f'time_series_sst_prediction_LSuperior_{TARGET_LAT_TS}_{TARGET_LON_TS}.csv')\n",
    "    ts_df.to_csv(ts_csv_filename)\n",
    "    print(f\"Saved: {ts_csv_filename}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Plotting the time series\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(ts_df.index, ts_df['mean'], label='Predicted Mean SST', color='blue')\n",
    "    plt.fill_between(ts_df.index,\n",
    "                     ts_df['mean'] - ts_df['std'],\n",
    "                     ts_df['mean'] + ts_df['std'],\n",
    "                     color='lightblue', alpha=0.6, label='Mean +/- Std Dev')\n",
    "\n",
    "    plt.title(f'SST Prediction Time Series at Lake Superior (Lat: {TARGET_LAT_TS}, Lon: {TARGET_LON_TS})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('SST (°C)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    ts_plot_filename = os.path.join(PREDICTION_OUTPUT_DIR, f'time_series_sst_prediction_LSuperior_{TARGET_LAT_TS}_{TARGET_LON_TS}.png')\n",
    "    plt.savefig(ts_plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {ts_plot_filename}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"Time series prediction completed.\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f57c6a-53b1-40a1-b7c3-626390c6732c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    " This script successfully loaded a trained DeepSensor model, performed\n",
    " a full-map prediction for a specific date, and generated a time series\n",
    " prediction for a single point in Lake Superior, including uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece550b-ffd3-415b-8569-8158b6c65028",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "\n",
    " Below we list some aspects of the computing environment for better reproduciblity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd259171-1792-47c5-9962-6cec76693386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Reproducibility Information ---\n",
      "Python Executable: /home/dannes/ds_env_gpu/bin/python\n",
      "Python Version: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Reproducibility Information ---\")\n",
    "print(\"Python Executable:\", sys.executable)\n",
    "print(\"Python Version:\", sys.version)\n",
    "sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
