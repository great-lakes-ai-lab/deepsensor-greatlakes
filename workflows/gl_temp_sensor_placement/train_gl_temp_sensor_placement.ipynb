{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f6eeae-bd6a-46d9-bc48-57697085c8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import dask.array as da\n",
    "import gcsfs\n",
    "import torch\n",
    "\n",
    "import deepsensor.torch  # noqa: F401 (enables torch backend)\n",
    "from deepsensor.data import DataProcessor, TaskLoader, construct_circ_time_ds\n",
    "from deepsensor.model import ConvNP\n",
    "from deepsensor.train import Trainer, set_gpu_default_device\n",
    "\n",
    "# Local package utilities\n",
    "from deepsensor_greatlakes.utils import (\n",
    "    standardize_dates,\n",
    "    generate_random_coordinates,\n",
    "    apply_mask_to_prediction,\n",
    ")\n",
    "from deepsensor_greatlakes.preprocessor import (\n",
    "    SeasonalCycleProcessor,\n",
    "    list_saved_seasonal_cycles,\n",
    ")\n",
    "from deepsensor_greatlakes.model import save_model, load_convnp_model\n",
    "\n",
    "set_gpu_default_device()\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9553f8-f18c-49f9-b3a6-9524fa16861e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in MODE = 'scaled'\n",
      "Config OK.\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Project / GCS configuration\n",
    "# ===============================================================\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "PROJECT_ID = \"great-lakes-osd\"\n",
    "LOCATION   = \"us-central1\"\n",
    "\n",
    "# Only store the bucket *name* here\n",
    "BUCKET_NAME = \"great-lakes-osd\"\n",
    "\n",
    "# Derive the gs:// form where needed\n",
    "BUCKET = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# Core datasets in GCS\n",
    "BATHYMETRY_PATH        = f\"{BUCKET}/context/gl_bathy_depth_3arcsec.nc\"\n",
    "LAKEMASK_PATH          = f\"{BUCKET}/context/lakemask.nc\"\n",
    "ICE_CONCENTRATION_PATH = f\"{BUCKET}/ice/ice_concentration_glsea3_838x1181_1995-2022.zarr\"\n",
    "GLSEA_PATH             = f\"{BUCKET}/GLSEA_combined.zarr\"\n",
    "\n",
    "# ===============================================================\n",
    "# Local folders for configs / models\n",
    "# ===============================================================\n",
    "\n",
    "# DataProcessor normalisation config\n",
    "DATAPROCESSOR_DIR = Path(\"./deepsensor_config\")\n",
    "DATAPROCESSOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Where to save the trained ConvNP model (best checkpoint)\n",
    "MODEL_DIR_LOCAL = Path(\"./models/convnp_sst\")\n",
    "MODEL_DIR_LOCAL.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# (Optional) String for a GCS model folder if you later want to sync/upload\n",
    "MODEL_DIR_GCS = f\"{BUCKET}/models/convnp_sst\"\n",
    "\n",
    "# ===============================================================\n",
    "# Experiment mode: 'debug', 'scaled', or 'full'\n",
    "# ===============================================================\n",
    "\n",
    "MODE = \"scaled\"   # options: \"debug\", \"scaled\", \"full\"\n",
    "print(f\"Running in MODE = {MODE!r}\")\n",
    "\n",
    "# ===============================================================\n",
    "# Time ranges + hyperparameters by mode\n",
    "# ===============================================================\n",
    "\n",
    "if MODE == \"debug\":\n",
    "    # Tiny, safe, for smoke tests\n",
    "    DATA_RANGE            = (\"2009-01-01\", \"2010-12-31\")\n",
    "    TRAIN_RANGE           = (\"2009-01-01\", \"2009-12-31\")\n",
    "    VAL_RANGE             = (\"2010-01-01\", \"2010-12-31\")\n",
    "    DATE_SUBSAMPLE_FACTOR = 30      # ~monthly\n",
    "\n",
    "    N_RANDOM_POINTS  = 50           # fewer buoys per task\n",
    "    N_EPOCHS         = 10\n",
    "    LEARNING_RATE    = 5e-5\n",
    "    BATCH_SIZE       = 1\n",
    "    INTERNAL_DENSITY = 200          # keep ConvNP cheap\n",
    "    SEED             = 42\n",
    "\n",
    "elif MODE == \"scaled\":\n",
    "    # First serious run on a T4, still conservative\n",
    "    # Match ice record: 1995â€“2022\n",
    "    DATA_RANGE            = (\"1995-01-01\", \"2022-12-31\")\n",
    "    TRAIN_RANGE           = (\"1995-01-01\", \"2016-12-31\")\n",
    "    VAL_RANGE             = (\"2017-01-01\", \"2022-12-31\")\n",
    "    DATE_SUBSAMPLE_FACTOR = 7       # weekly sampling\n",
    "\n",
    "    N_RANDOM_POINTS  = 150          # N context buoys per task\n",
    "    N_EPOCHS         = 50\n",
    "    LEARNING_RATE    = 5e-5\n",
    "    BATCH_SIZE       = 1            # can try 2 later if GPU is comfy\n",
    "    INTERNAL_DENSITY = 400          # bigger than debug, still T4-friendly\n",
    "    SEED             = 42\n",
    "\n",
    "elif MODE == \"full\":\n",
    "    # Ambitious run; may need bigger GPU or at least monitoring\n",
    "    DATA_RANGE            = (\"1995-01-01\", \"2022-12-31\")\n",
    "    TRAIN_RANGE           = (\"1995-01-01\", \"2018-12-31\")\n",
    "    VAL_RANGE             = (\"2019-01-01\", \"2022-12-31\")\n",
    "    DATE_SUBSAMPLE_FACTOR = 3       # every 3rd day\n",
    "\n",
    "    N_RANDOM_POINTS  = 200\n",
    "    N_EPOCHS         = 75\n",
    "    LEARNING_RATE    = 5e-5\n",
    "    BATCH_SIZE       = 1            # bump to 2 only if VRAM is clearly free\n",
    "    INTERNAL_DENSITY = 600          # increase cautiously; main GPU knob\n",
    "    SEED             = 42\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODE={MODE!r}\")\n",
    "\n",
    "# =====================================================================\n",
    "# CONFIG: Should we LOAD an existing DataProcessor, or BUILD a new one?\n",
    "# =====================================================================\n",
    "USE_EXISTING_DATAPROCESSOR = True   # Set to False to rebuild from scratch\n",
    "\n",
    "# ===============================================================\n",
    "# Target variable and seeding\n",
    "# ===============================================================\n",
    "\n",
    "# Name of the GLSEA variable to predict \n",
    "TARGET_VAR_NAME = \"sst\"    # change to \"sst_anom\" etc. if needed\n",
    "\n",
    "# Set seeds for reproducibility-ish behaviour\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Config OK.\")\n",
    "print(f\"TRAIN_RANGE={TRAIN_RANGE}, VAL_RANGE={VAL_RANGE}, \"\n",
    "      f\"DATE_SUBSAMPLE_FACTOR={DATE_SUBSAMPLE_FACTOR}, \"\n",
    "      f\"N_RANDOM_POINTS={N_RANDOM_POINTS}, INTERNAL_DENSITY={INTERNAL_DENSITY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25cbbf1-c62f-4e01-8f8f-9b68d9b9719c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Zarr datasets from GCS...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ICE_CONCENTRATION_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ===============================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Load temporal datasets (ice concentration, GLSEA)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ===============================================================\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpening Zarr datasets from GCS...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m ice_concentration_raw \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_zarr(\u001b[43mICE_CONCENTRATION_PATH\u001b[49m)\n\u001b[1;32m      7\u001b[0m glsea_raw \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_zarr(GLSEA_PATH)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Replace sentinel land value -1 with NaN in ice concentration\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ICE_CONCENTRATION_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Load temporal datasets (ice concentration, GLSEA)\n",
    "# ===============================================================\n",
    "\n",
    "print(\"Opening Zarr datasets from GCS...\")\n",
    "ice_concentration_raw = xr.open_zarr(ICE_CONCENTRATION_PATH)\n",
    "glsea_raw = xr.open_zarr(GLSEA_PATH)\n",
    "\n",
    "# Replace sentinel land value -1 with NaN in ice concentration\n",
    "ice_concentration_raw = ice_concentration_raw.where(ice_concentration_raw != -1, float(\"nan\"))\n",
    "\n",
    "# Drop the useless CRS variable if present\n",
    "if \"crs\" in glsea_raw:\n",
    "    glsea_raw = glsea_raw.drop_vars(\"crs\")\n",
    "\n",
    "# Convert times to date-only\n",
    "ice_concentration_raw = standardize_dates(ice_concentration_raw)\n",
    "glsea_raw = standardize_dates(glsea_raw)\n",
    "\n",
    "# Restrict to chosen data range\n",
    "t0, t1 = DATA_RANGE\n",
    "ice_concentration_raw = ice_concentration_raw.sel(time=slice(t0, t1))\n",
    "glsea_raw = glsea_raw.sel(time=slice(t0, t1))\n",
    "\n",
    "print(\"ice_concentration:\", ice_concentration_raw)\n",
    "print(\"glsea:\", glsea_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40610c4-5363-4d9c-8309-b0a7d204fda4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Load static datasets (bathymetry + lake mask) from GCS via gcsfs\n",
    "# ===============================================================\n",
    "\n",
    "print(\"Opening bathymetry + lake mask from GCS using gcsfs...\")\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "# gcsfs paths must be \"bucket/path\", not \"gs://bucket/path\"\n",
    "bathy_path_gcsfs = f\"{BUCKET_NAME}/context/gl_bathy_depth_3arcsec.nc\"\n",
    "mask_path_gcsfs  = f\"{BUCKET_NAME}/context/lakemask.nc\"\n",
    "\n",
    "# IMPORTANT: Do NOT close these file objects â€” xarray needs them\n",
    "bathy_fobj = fs.open(bathy_path_gcsfs, \"rb\")\n",
    "mask_fobj  = fs.open(mask_path_gcsfs,  \"rb\")\n",
    "\n",
    "bathymetry_raw = xr.open_dataset(bathy_fobj)\n",
    "lakemask_raw   = xr.open_dataset(mask_fobj)\n",
    "\n",
    "# ===============================================================\n",
    "# Clean up bathymetry + lake mask datasets\n",
    "# ===============================================================\n",
    "\n",
    "# --- Clean bathymetry_raw ---\n",
    "# Drop the useless CRS variable if present\n",
    "if \"crs\" in bathymetry_raw:\n",
    "    bathymetry_raw = bathymetry_raw.drop_vars(\"crs\")\n",
    "\n",
    "# Rename Band1 â†’ depth (this will become much nicer downstream)\n",
    "if \"Band1\" in bathymetry_raw:\n",
    "    bathymetry_raw = bathymetry_raw.rename({\"Band1\": \"depth\"})\n",
    "\n",
    "print(\"bathymetry_raw:\", bathymetry_raw)\n",
    "print(\"lakemask_raw:\", lakemask_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd338f4-f1d6-49c5-92db-9baa7d9c81df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Construct circular day-of-year features\n",
    "# ===============================================================\n",
    "\n",
    "from deepsensor.data import construct_circ_time_ds\n",
    "\n",
    "# Build a date coordinate covering the full GLSEA dataset\n",
    "dates = pd.date_range(\n",
    "    glsea_raw.time.values.min(),\n",
    "    glsea_raw.time.values.max(),\n",
    "    freq=\"D\"\n",
    ")\n",
    "\n",
    "# Construct circular time dataset (cosine + sine of day-of-year)\n",
    "# This dataset has its own time index matching \"dates\"\n",
    "doy_ds = construct_circ_time_ds(dates, freq=\"D\")\n",
    "\n",
    "# Standardize times so they match the YYYY-MM-DD format everywhere else\n",
    "cos_D = standardize_dates(doy_ds[\"cos_D\"])\n",
    "sin_D = standardize_dates(doy_ds[\"sin_D\"])\n",
    "\n",
    "print(\"Circular time features constructed:\")\n",
    "print(\"cos_D:\", cos_D)\n",
    "print(\"sin_D:\", sin_D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3aa3a5-04d6-4238-a72b-ab2675b60411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# DataProcessor Setup: Register all variables with correct scaling\n",
    "# ===============================================================\n",
    "#\n",
    "# DeepSensor's DataProcessor learns normalization the FIRST time it\n",
    "# sees each dataset. We therefore either:\n",
    "#   - LOAD an existing, saved DataProcessor (preferred), or\n",
    "#   - BUILD a new one and save it for future runs.\n",
    "#\n",
    "# Scaling strategy:\n",
    "#   - SST (glsea) ....................... mean_std   (Gaussian-ish field)\n",
    "#   - Ice Concentration ................. min_max    (bounded 0â€“1)\n",
    "#   - Bathymetry ........................ min_max    (static absolute field)\n",
    "#   - Lake Mask ......................... min_max    (binary 0/1, preserved)\n",
    "#\n",
    "# After registration, we process the full datasets with consistent scaling.\n",
    "# ===============================================================\n",
    "\n",
    "print(\"=== DataProcessor setup ===\")\n",
    "\n",
    "# 0. Attempt to LOAD existing DataProcessor, or BUILD a new one\n",
    "if USE_EXISTING_DATAPROCESSOR and Path(DATAPROCESSOR_DIR).exists():\n",
    "    print(f\"Loading existing DataProcessor from: {DATAPROCESSOR_DIR}\")\n",
    "    data_processor = DataProcessor(DATAPROCESSOR_DIR)\n",
    "    USING_LOADED_PROCESSOR = True\n",
    "\n",
    "else:\n",
    "    print(\"Building a NEW DataProcessor...\")\n",
    "    data_processor = DataProcessor(x1_name=\"lat\", x2_name=\"lon\")\n",
    "    USING_LOADED_PROCESSOR = False\n",
    "\n",
    "    # 1. Register SST (glsea) with MEAN-STD scaling\n",
    "    print(\"  Registering SST (glsea) with mean_std scaling...\")\n",
    "    _ = data_processor(glsea_raw, method=\"mean_std\")\n",
    "\n",
    "    # 2. Register Ice Concentration with MIN-MAX scaling\n",
    "    print(\"  Registering Ice Concentration with min_max scaling...\")\n",
    "    _ = data_processor(ice_concentration_raw, method=\"min_max\")\n",
    "\n",
    "    # 3. Register Bathymetry with MIN-MAX scaling\n",
    "    print(\"  Registering bathymetry with min_max scaling...\")\n",
    "    _ = data_processor(bathymetry_raw, method=\"min_max\")\n",
    "\n",
    "    # 4. Register Lake Mask with MIN-MAX scaling\n",
    "    print(\"  Registering lake mask with min_max scaling...\")\n",
    "    _ = data_processor(lakemask_raw, method=\"min_max\")\n",
    "\n",
    "    print(\"\\nRegistration complete.\")\n",
    "    print(\"Registered variables:\")\n",
    "    for name, cfg in data_processor.config.items():\n",
    "        print(f\"  - {name}: {cfg}\")\n",
    "\n",
    "    # Save the learned scaling once\n",
    "    data_processor.save(DATAPROCESSOR_DIR)\n",
    "    print(f\"\\nSaved new DataProcessor to: {DATAPROCESSOR_DIR}\")\n",
    "\n",
    "if USING_LOADED_PROCESSOR:\n",
    "    print(\"Using previously-saved normalization parameters.\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Process all datasets with learned scaling\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\nApplying DataProcessor scaling to all datasets...\")\n",
    "\n",
    "glsea             = data_processor(glsea_raw)\n",
    "ice_concentration = data_processor(ice_concentration_raw)\n",
    "bathymetry        = data_processor(bathymetry_raw)\n",
    "lakemask          = data_processor(lakemask_raw)\n",
    "\n",
    "print(\"\\nDataProcessor processing complete.\")\n",
    "print(f\"  glsea processed vars:             {list(glsea.data_vars)}\")\n",
    "print(f\"  ice_concentration processed vars: {list(ice_concentration.data_vars)}\")\n",
    "print(f\"  bathymetry processed vars:        {list(bathymetry.data_vars)}\")\n",
    "print(f\"  lakemask processed vars:          {list(lakemask.data_vars)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b574db-42a7-493f-bd2f-3acac8e413f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# TaskLoader setup and task generation\n",
    "# ===============================================================\n",
    "#\n",
    "# We:\n",
    "#   1. Build an auxiliary context dataset (static + time features)\n",
    "#   2. Instantiate a TaskLoader with:\n",
    "#        - context = [glsea, ice_concentration, aux_ds]\n",
    "#        - target  = glsea (SST field)\n",
    "#   3. Define a gen_tasks(...) helper to:\n",
    "#        - sample N random lake points per date\n",
    "#        - build tasks\n",
    "#        - remove NaNs from targets\n",
    "#   4. Generate train/val tasks\n",
    "# ===============================================================\n",
    "\n",
    "# 1. Auxiliary context dataset\n",
    "#    Note: bathymetry var is now \"depth\", mask var is \"mask\".\n",
    "aux_ds = xr.Dataset({\n",
    "    \"mask\":  lakemask[\"mask\"],      # processed lake mask (0/1)\n",
    "    \"cos_D\": cos_D,                 # circular time features (already sin/cos)\n",
    "    \"sin_D\": sin_D,\n",
    "})\n",
    "\n",
    "print(\"aux_ds:\", aux_ds)\n",
    "\n",
    "# 2. TaskLoader: contexts and target\n",
    "task_loader = TaskLoader(\n",
    "    context=[glsea, ice_concentration, bathymetry, aux_ds],\n",
    "    target=glsea,\n",
    ")\n",
    "\n",
    "print(\"\\nTaskLoader initialised:\")\n",
    "print(task_loader)\n",
    "\n",
    "\n",
    "# 3. Task generation helper\n",
    "def gen_tasks(dates, N=100, progress=True, lakemask_raw=None, data_processor=None):\n",
    "    \"\"\"\n",
    "    Generate a list of DeepSensor tasks for given dates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dates : iterable of np.datetime64 / pandas.Timestamp\n",
    "        Dates at which to sample tasks.\n",
    "    N : int\n",
    "        Number of random lake points (context locations) per task.\n",
    "    progress : bool\n",
    "        If True, show tqdm progress bar.\n",
    "    lakemask_raw : xr.Dataset\n",
    "        Raw lake mask dataset used by `generate_random_coordinates`.\n",
    "    data_processor : deepsensor.data.DataProcessor\n",
    "        DataProcessor instance used to normalise coordinates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tasks : list\n",
    "        List of DeepSensor Task objects.\n",
    "    \"\"\"\n",
    "    if lakemask_raw is None or data_processor is None:\n",
    "        raise ValueError(\"You must pass both `lakemask_raw` and `data_processor`.\")\n",
    "\n",
    "    tasks = []\n",
    "    for date in tqdm(dates, disable=not progress, desc=\"Generating tasks\"):\n",
    "        # Generate a fresh set of random lake points for each date\n",
    "        random_points = generate_random_coordinates(lakemask_raw, N, data_processor)\n",
    "\n",
    "        # Sample the task:\n",
    "        #   - context_sampling: use our N random coordinates\n",
    "        #   - target_sampling:  \"all\" â†’ full SST field on that date\n",
    "        task = task_loader(date, context_sampling=random_points, target_sampling=\"all\")\n",
    "\n",
    "        # Remove NaNs from the target (e.g., land, missing data)\n",
    "        task = task.remove_target_nans()\n",
    "\n",
    "        tasks.append(task)\n",
    "\n",
    "    return tasks\n",
    "\n",
    "\n",
    "# 4. Build train/validation date lists\n",
    "#    (Assumes TRAIN_RANGE, VAL_RANGE, DATE_SUBSAMPLE_FACTOR are already defined,\n",
    "#     e.g.: TRAIN_RANGE = (\"2009-01-01\",\"2009-12-31\"), etc.)\n",
    "\n",
    "train_dates = pd.date_range(TRAIN_RANGE[0], TRAIN_RANGE[1])[::DATE_SUBSAMPLE_FACTOR]\n",
    "val_dates   = pd.date_range(VAL_RANGE[0],   VAL_RANGE[1])[::DATE_SUBSAMPLE_FACTOR]\n",
    "\n",
    "# Normalize to datetime64[D]\n",
    "train_dates = pd.to_datetime(train_dates).normalize()\n",
    "val_dates   = pd.to_datetime(val_dates).normalize()\n",
    "\n",
    "print(f\"\\nNumber of training dates:   {len(train_dates)}\")\n",
    "print(f\"Number of validation dates: {len(val_dates)}\")\n",
    "\n",
    "\n",
    "# 5. Generate training and validation tasks\n",
    "train_tasks = gen_tasks(\n",
    "    train_dates,\n",
    "    N=100,  # number of random lake points per task\n",
    "    lakemask_raw=lakemask_raw,\n",
    "    data_processor=data_processor,\n",
    ")\n",
    "\n",
    "val_tasks = gen_tasks(\n",
    "    val_dates,\n",
    "    N=100,\n",
    "    lakemask_raw=lakemask_raw,\n",
    "    data_processor=data_processor,\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(train_tasks)} training tasks and {len(val_tasks)} validation tasks.\")\n",
    "\n",
    "\n",
    "# 6. Quick sanity check plot of one task\n",
    "fig = deepsensor.plot.task(train_tasks[2], task_loader)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30bebb5-a09f-4dde-b7ce-731a79a41cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 1. Device + model + trainer\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "print(\"Initialising ConvNP model and Trainer...\")\n",
    "model = ConvNP(\n",
    "    data_processor, \n",
    "    task_loader,\n",
    "    internal_density=INTERNAL_DENSITY,\n",
    ")\n",
    "\n",
    "# Trainer only takes (model, lr)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# For un-normalising SST later\n",
    "target_var_ID = task_loader.target_var_IDs[0][0]  # first target, 1D\n",
    "\n",
    "print(\"Model and Trainer ready.\")\n",
    "print(f\"Training for {N_EPOCHS} epochs with lr={LEARNING_RATE:g}, batch_size={BATCH_SIZE}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Helper: validation RMSE in physical SST units\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def compute_val_rmse(model, val_tasks, task_loader, data_processor, target_var_ID):\n",
    "    \"\"\"\n",
    "    Compute RMSE over all validation tasks in un-normalised SST units.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for task in val_tasks:\n",
    "        with torch.no_grad():\n",
    "            # Mean prediction in normalised space\n",
    "            mean_norm = model.mean(task)\n",
    "\n",
    "            # Map prediction and truth back to physical SST\n",
    "            mean = data_processor.map_array(\n",
    "                mean_norm,\n",
    "                target_var_ID,\n",
    "                unnorm=True,\n",
    "            )\n",
    "            true = data_processor.map_array(\n",
    "                task[\"Y_t\"][0],\n",
    "                target_var_ID,\n",
    "                unnorm=True,\n",
    "            )\n",
    "\n",
    "        diff = mean - true\n",
    "        errors.append(diff.ravel() ** 2)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    errors = np.concatenate(errors)\n",
    "    rmse = float(np.sqrt(np.mean(errors)))\n",
    "    return rmse\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Training loop (now using Trainer.__call__ with batch_size)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "losses        = []\n",
    "val_rmses     = []\n",
    "best_val_rmse = np.inf\n",
    "\n",
    "print(\"\\n=== Starting training loop ===\")\n",
    "\n",
    "for epoch in tqdm(range(1, N_EPOCHS + 1), desc=\"Training epochs\"):\n",
    "    # ðŸ”¹ IMPORTANT: pass batch_size here to avoid OOM\n",
    "    batch_losses = trainer(\n",
    "        train_tasks,\n",
    "        batch_size=BATCH_SIZE,      # this is the DeepSensor-supported batching\n",
    "        progress_bar=False,\n",
    "        tqdm_notebook=False,\n",
    "    )\n",
    "    epoch_loss = float(np.mean(batch_losses))\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Validation RMSE in physical SST units\n",
    "    val_rmse = compute_val_rmse(\n",
    "        model,\n",
    "        val_tasks,\n",
    "        task_loader,\n",
    "        data_processor,\n",
    "        target_var_ID,\n",
    "    )\n",
    "    val_rmses.append(val_rmse)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | loss = {epoch_loss:.4f} | val RMSE = {val_rmse:.4f}\")\n",
    "\n",
    "    # Save best model so far\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        save_model(model, MODEL_DIR_LOCAL)\n",
    "        print(f\"  â†³ New best model saved to {MODEL_DIR_LOCAL} (RMSE = {best_val_rmse:.4f})\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(f\"Best validation RMSE: {best_val_rmse:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Plot training diagnostics\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Training loss\")\n",
    "axes[0].set_title(\"Training loss\")\n",
    "\n",
    "axes[1].plot(val_rmses)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Validation RMSE\")\n",
    "axes[1].set_title(\"Validation RMSE\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb008714-e23f-4fb2-abd3-669ca07c5c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Single prediction on one validation date\n",
    "# ===============================================================\n",
    "\n",
    "from deepsensor_greatlakes.model import load_convnp_model\n",
    "\n",
    "# 1. Reload best model from disk (to confirm save/load works)\n",
    "print(f\"Loading best model from {MODEL_DIR_LOCAL} ...\")\n",
    "best_model = load_convnp_model(MODEL_DIR_LOCAL, data_processor, task_loader)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# 2. Choose a prediction date (first validation date, for example)\n",
    "PRED_DATE = val_dates[0]\n",
    "print(f\"Making prediction for date: {PRED_DATE}\")\n",
    "\n",
    "# 3. Sample context points for this date\n",
    "context_points = generate_random_coordinates(\n",
    "    lakemask_raw,\n",
    "    N_RANDOM_POINTS,\n",
    "    data_processor,\n",
    ")\n",
    "\n",
    "# 4. Build a task for this date\n",
    "test_task = task_loader(\n",
    "    PRED_DATE,\n",
    "    context_sampling=context_points,\n",
    "    target_sampling=\"all\",\n",
    ")\n",
    "\n",
    "# 5. Run prediction.\n",
    "#    X_t argument defines where we want the output grid; using raw GLSEA grid here.\n",
    "pred_ds = best_model.predict(test_task, X_t=glsea_raw)\n",
    "\n",
    "print(\"Prediction dataset:\")\n",
    "print(pred_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3591b-3698-4a10-9c8f-001efb077cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Mask prediction to lake and plot mean + std\n",
    "# ===============================================================\n",
    "\n",
    "# Extract the SST field from the prediction dataset\n",
    "sst_pred = pred_ds[TARGET_VAR_NAME]   # \"sst\"\n",
    "\n",
    "# Apply lake mask (your helper handles matching coords)\n",
    "sst_pred_masked = apply_mask_to_prediction(sst_pred, lakemask_raw)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Mean\n",
    "plt.subplot(1, 2, 1)\n",
    "sst_pred_masked[\"mean\"].plot(\n",
    "    cmap=\"viridis\",\n",
    "    cbar_kwargs={\"label\": \"Predicted SST (Â°C)\"},\n",
    ")\n",
    "plt.title(f\"Predicted mean SST\\n{np.array(PRED_DATE).astype('datetime64[D]')}\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# Std (uncertainty)\n",
    "plt.subplot(1, 2, 2)\n",
    "sst_pred_masked[\"std\"].plot(\n",
    "    cmap=\"plasma\",\n",
    "    cbar_kwargs={\"label\": \"Predictive std (Â°C)\"},\n",
    ")\n",
    "plt.title(f\"Predictive uncertainty (std)\\n{np.array(PRED_DATE).astype('datetime64[D]')}\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1873188-3dd8-4708-b793-56de427149af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "venv",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python (deepsensor_env_gpu) (Local)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
